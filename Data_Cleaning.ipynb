{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nimport boto3\nimport json\nfrom awsglue.transforms import *\nfrom pyspark.sql.functions import rand\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,when,expr,current_timestamp\nfrom datetime import datetime\nfrom pyspark.sql.types import TimestampType\nfrom io import StringIO\n\n# Set up Spark session\nspark = SparkSession.builder.appName(\"datacleansing\").getOrCreate()\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 62,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session e5006eaa-df4d-4ec6-8217-162d1e534005.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session e5006eaa-df4d-4ec6-8217-162d1e534005.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 4.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session e5006eaa-df4d-4ec6-8217-162d1e534005.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session e5006eaa-df4d-4ec6-8217-162d1e534005.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: 5\nSetting new number of workers to: 5\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Set your AWS credentials and region\naws_access_key_id = \"AKIAVL7N3YDQP6O557AN\"\naws_secret_access_key = \"MvSsreqywSnvpwuEZBHRRKIBh2ideMZn+kNKOKwO\"\nregion = 'us-west-1'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Set up S3 client\ns3_client = boto3.client(\"s3\", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Get the most file paths of most recent file to perform cleansing on\ndef get_filename_for_cleaning(dataset, historical=False):\n    if historical:\n        if dataset == \"Crashes\":\n            return \"s3://uncharted-s3/raw/historical_data/Traffic_Crashes.csv\"\n        elif dataset == \"Vehicles\":\n            return \"s3://uncharted-s3/raw/historical_data/Crashes_Vehicles.csv\"\n        elif dataset == \"People\":\n            return \"s3://uncharted-s3/raw/historical_data/Crashes_People.csv\"\n        else:\n            print('Invalid dataset name')\n    else:\n        if dataset == \"Crashes\":\n            #List all the files in incremental Crashes raw folder\n            objects = s3_client.list_objects_v2(Bucket='uncharted-s3',Prefix='raw/incremental/Crashes/')[\"Contents\"]\n            # Get the most recent Crashes data\n            most_recent_object = max((obj for obj in objects if obj[\"Key\"].startswith(\"raw/incremental/Crashes/\") and obj[\"Key\"].endswith(\".csv\")), key=lambda x: x[\"LastModified\"])\n            #Return the path to the most recent file\n            return \"s3://uncharted-s3/{}\".format(most_recent_object[\"Key\"])\n        elif dataset == \"Vehicles\":\n            #List all the files in incremental Vehicles raw folder\n            objects = s3_client.list_objects_v2(Bucket='uncharted-s3',Prefix='raw/incremental/Vehicles/')[\"Contents\"]\n            # Get the most recent Vehicles data\n            most_recent_object = max((obj for obj in objects if obj[\"Key\"].startswith(\"raw/incremental/Vehicles/\") and obj[\"Key\"].endswith(\".csv\")), key=lambda x: x[\"LastModified\"])\n            #Return the path to the most recent file\n            return \"s3://uncharted-s3/{}\".format(most_recent_object[\"Key\"])\n        elif dataset == \"People\":\n            #List all the files in incremental People raw folder\n            objects = s3_client.list_objects_v2(Bucket='uncharted-s3',Prefix='raw/incremental/People/')[\"Contents\"]\n            # Get the most recent Crashes data\n            most_recent_object = max((obj for obj in objects if obj[\"Key\"].startswith(\"raw/incremental/People/\") and obj[\"Key\"].endswith(\".csv\")), key=lambda x: x[\"LastModified\"])\n            #Return the path to the most recent file\n            return \"s3://uncharted-s3/{}\".format(most_recent_object[\"Key\"])   \n        else:\n            print('Invalid dataset name')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Clean the raw data to be used for transformation\ndef clean_data(dataset,historical=False):\n    if dataset == 'Crashes':\n        df = spark.read.csv(get_filename_for_cleaning(dataset,historical), header=True, inferSchema=True)\n        #Drop rows with all null values\n        df = df.na.drop(how='all')\n        #Drop duplicate rows\n        df = df.dropDuplicates()\n        #Drop unwanted columns\n        columns_to_drop=['location','crash_date_est_i']\n        cleansed_df = df.drop(*columns_to_drop)\n        return cleansed_df\n    elif dataset == 'Vehicles':\n        df = spark.read.csv(get_filename_for_cleaning(dataset,historical), header=True, inferSchema=True)\n        #Drop rows with all null values\n        df = df.na.drop(how='all')\n        #Drop duplicate rows\n        df = df.dropDuplicates()\n        #Drop unwanted columns\n        columns_to_drop=['cmrc_veh_i','fire_i','area_00_i','area_01_i','area_02_i','area_03_i','area_04_i','area_05_i','area_06_i','area_07_i','area_08_i','area_09_i','area_10_i','area_11_i','area_12_i','area_99_i','cmv_id','usdot_no','ccmc_no','ilcc_no','commercial_src','gvwr','carrier_name','carrier_state','carrier_city','hazmat_placards_i','hazmat_name','un_no','hazmat_present_i','hazmat_report_i','hazmat_report_no','mcs_report_i','mcs_report_no','hazmat_vio_cause_crash_i','mcs_vio_cause_crash_i','idot_permit_no','wide_load_i','trailer1_width','trailer2_width','trailer1_length','trailer2_length','total_vehicle_length','axle_cnt','vehicle_config','cargo_body_type','load_type','hazmat_out_of_service_i','mcs_out_of_service_i','hazmat_class']\n        cleansed_df = df.drop(*columns_to_drop)\n        return cleansed_df\n    elif dataset == 'People':\n        df = spark.read.csv(get_filename_for_cleaning(dataset,historical), header=True, inferSchema=True)\n        #Drop rows with all null values\n        df = df.na.drop(how='all')\n        #Drop duplicate rows\n        df = df.dropDuplicates()\n        #Replace X with blank in Gender column\n        cleansed_df = df.withColumn(\"sex\", when(df[\"sex\"] == \"x\", \"\").otherwise(df[\"sex\"]))\n        return cleansed_df",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#file paths to store the cleansed data in S3 clean folder\ndef get_destination_file_path(datasets,historical=False):\n    if historical:\n        if datasets == 'Crashes':\n            return \"clean/historical_data/clean_Traffic_Crashes.csv\"\n        elif datasets == 'Vehicles':\n            return \"clean/historical_data/clean_Crashes_Vehicles.csv\"\n        elif datasets == 'People':\n            return \"clean/historical_data/clean_Crashes_People.csv\"\n        else:\n            print('Invalid dataset name')\n    else:\n        if datasets == 'Crashes':\n            #List all the files in incremental Crashes raw folder\n            objects = s3_client.list_objects_v2(Bucket='uncharted-s3',Prefix='raw/incremental/Crashes/')[\"Contents\"]\n            # Get the most recent Crashes data\n            most_recent_object = max((obj for obj in objects if obj[\"Key\"].startswith(\"raw/incremental/Crashes/\") and obj[\"Key\"].endswith(\".csv\")), key=lambda x: x[\"LastModified\"])\n            most_recent_object = most_recent_object[\"Key\"]\n            #Return the filename to be stored in\n            return most_recent_object.replace('raw', 'clean').replace('Crashes_','clean_Crashes_')\n        elif datasets == 'Vehicles':\n            #List all the files in incremental Crashes raw folder\n            objects = s3_client.list_objects_v2(Bucket='uncharted-s3',Prefix='raw/incremental/Vehicles/')[\"Contents\"]\n            # Get the most recent Crashes data\n            most_recent_object = max((obj for obj in objects if obj[\"Key\"].startswith(\"raw/incremental/Vehicles/\") and obj[\"Key\"].endswith(\".csv\")), key=lambda x: x[\"LastModified\"])\n            most_recent_object = most_recent_object[\"Key\"]\n            #Return the filename to be stored in\n            return most_recent_object.replace('raw', 'clean').replace('Vehicles_','clean_Vehicles_')\n        elif datasets == 'People':\n            #List all the files in incremental Crashes raw folder\n            objects = s3_client.list_objects_v2(Bucket='uncharted-s3',Prefix='raw/incremental/People/')[\"Contents\"]\n            # Get the most recent Crashes data\n            most_recent_object = max((obj for obj in objects if obj[\"Key\"].startswith(\"raw/incremental/People/\") and obj[\"Key\"].endswith(\".csv\")), key=lambda x: x[\"LastModified\"])\n            most_recent_object = most_recent_object[\"Key\"]\n            #Return the filename to be stored in\n            return most_recent_object.replace('raw', 'clean').replace('People_','clean_People_')\n        else:\n            print('Invalid dataset name')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 57,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Write the cleansed data into S3 clean folder\ndef upload_csv_to_s3(df, bucket_name, object_key):\n    # Create an S3 client\n    s3 = boto3.client('s3')\n\n    # Upload the CSV data to S3\n    try:\n        # Convert DataFrame to CSV format\n        csv_data = df.toPandas().to_csv(index=False)\n\n        # Upload the CSV data as an S3 object\n        s3.put_object(Body=csv_data, Bucket=bucket_name, Key=object_key)\n\n        print(f\"CSV data uploaded successfully to s3://{bucket_name}/{object_key}\")\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 66,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Clean the data for the three datasets\ndatasets = ['Crashes','Vehicles','People']\n#Set the historical_data variable to True only if you want to clean the historical dataset\nhistorical_data = False\n#S3 bucket name\nbucket_name = 'uncharted-s3'\nfor dataset in datasets:\n    #Clean the raw data\n    cleansed_df = clean_data(dataset,historical_data)\n    #Get the path where the cleansed data should be stored\n    destination_path = get_destination_file_path(dataset,historical_data)\n    #Write the cleansed data into S3 clean folder\n    upload_csv_to_s3(cleansed_df, bucket_name, destination_path)\n    ",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 68,
			"outputs": [
				{
					"name": "stdout",
					"text": "CSV data uploaded successfully to s3://uncharted-s3/clean/historical_data/clean_Traffic_Crashes.csv\nCSV data uploaded successfully to s3://uncharted-s3/clean/historical_data/clean_Crashes_Vehicles.csv\nCSV data uploaded successfully to s3://uncharted-s3/clean/historical_data/clean_Crashes_People.csv\n",
					"output_type": "stream"
				}
			]
		}
	]
}