{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.context import SparkContext\nfrom pyspark.sql import SparkSession\nimport boto3\nimport json\nfrom awsglue.transforms import *\nfrom pyspark.sql.functions import rand\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,when,expr,current_timestamp\nfrom datetime import datetime\nfrom pyspark.sql.types import TimestampType\n# Set up Spark session\nspark = SparkSession.builder.appName(\"datatransformation\").getOrCreate()\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.2 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 845ca07c-8171-4d3c-b448-c3d8dc9d2a8d\nApplying the following default arguments:\n--glue_kernel_version 1.0.2\n--enable-glue-datacatalog true\nWaiting for session 845ca07c-8171-4d3c-b448-c3d8dc9d2a8d to get into ready status...\nSession 845ca07c-8171-4d3c-b448-c3d8dc9d2a8d has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Set your AWS credentials and region\naws_access_key_id = \"AKIAVL7N3YDQP6O557AN\"\naws_secret_access_key = \"MvSsreqywSnvpwuEZBHRRKIBh2ideMZn+kNKOKwO\"\nregion = 'us-west-1'",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Set up S3 client\ns3_client = boto3.client(\"s3\", aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "def data_transformation():\n    #Load the cleansed data into dataframes for transformation\n    crashes_df = spark.read.csv(\"s3://uncharted-s3/clean/historical_data/clean_Traffic_Crashes.csv\",header=True, inferSchema=True)\n    vehicles_df = spark.read.csv(\"s3://uncharted-s3/clean/historical_data/clean_Crashes_Vehicles.csv\",header=True, inferSchema=True)\n    people_df = spark.read.csv(\"s3://uncharted-s3/clean/historical_data/clean_Crashes_People.csv\",header=True, inferSchema=True)\n    \n    # Drop the 'crash date' column from vehicles_df since it is present in crashes_df\n    vehicles_df = vehicles_df.drop(\"CRASH_DATE\")\n    # Drop the 'crash date' column from people_df since it is present in crashes_df\n    people_df = people_df.drop(\"CRASH_DATE\")\n    return crashes_df, vehicles_df, people_df",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "#Path where the transformed data should be stored\ndest_path = \"s3://uncharted-s3/transformed/\"\n# Call the data_transformation function\ncrashes_df, vehicles_df, people_df = data_transformation()\n\n# Coalesce to a single partition before writing\ncrashes_df.write.csv(dest_path + \"crashes\", header=True, mode=\"overwrite\")\nvehicles_df.write.csv(dest_path + \"vehicles\", header=True, mode=\"overwrite\")\npeople_df.write.csv(dest_path + \"people\", header=True, mode=\"overwrite\")\n\nprint(\"Successfully wrote the transformed data into S3 bucket at:\", dest_path)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Successfully wrote the transformed data into S3 bucket at:  s3://uncharted-s3/transformed/\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}